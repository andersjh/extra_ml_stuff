{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Machine Learning Example\n",
    "\n",
    "###  This example reflects what a simple manual process for comming up witn an effective model for a classificaiton problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE FUNCTION\n",
    "\n",
    "Since we will be trying lots of different models, it would be nice to have a single function that will evaluate all our models and provide a standardized reporting format.\n",
    "\n",
    "This will allow us to easily pick out the model we want to move forward with.\n",
    "\n",
    "This function takes in a model ( pipeline ) and our train test split data. From there it simply performes predictions and generates results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pipeline, X_train, X_test, y_train, y_test):\n",
    "    '''\n",
    "    Evaluate a pipeline on training and test datasets\n",
    "    '''    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_train_hat = pipeline.predict(X_train)\n",
    "    y_test_hat = pipeline.predict(X_test)\n",
    "    train_f1 = f1_score(y_train_hat, y_train)\n",
    "    train_acc = accuracy_score(y_train_hat, y_train)\n",
    "    test_f1 = f1_score(y_test_hat, y_test)\n",
    "    test_acc = accuracy_score(y_test_hat, y_test)\n",
    "\n",
    "    print(f\"========== Predictor: {type(pipeline).__name__} ==========\")\n",
    "    print(f\"Training result: f1: {train_f1:.3f}, acc: {train_acc:.3f}\")\n",
    "    print(f\"Test result: f1: {test_f1:.3f}, acc: {test_acc:.3f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA\n",
    "\n",
    "In this case we are reading in transfusion data.  With this data we are trying to predict in an individual has given blood on Marth 2007 based on specific features.\n",
    "\n",
    "#### The features are:\n",
    "- Recency  ->   How long since the individual last gave blood\n",
    "- Frequency -> How many times has the indivaul given blood\n",
    "- Monetary -> Amount of usable blood given\n",
    "- Time -> How many months have they been giving blood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>12500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3250</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>5000</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>6000</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1750</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3000</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2250</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>11500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recency (months)  Frequency (times)  Monetary (c.c. blood)  Time (months)  \\\n",
       "0                 2                 50                  12500             98   \n",
       "1                 0                 13                   3250             28   \n",
       "2                 1                 16                   4000             35   \n",
       "3                 2                 20                   5000             45   \n",
       "4                 1                 24                   6000             77   \n",
       "5                 4                  4                   1000              4   \n",
       "6                 2                  7                   1750             14   \n",
       "7                 1                 12                   3000             35   \n",
       "8                 2                  9                   2250             22   \n",
       "9                 5                 46                  11500             98   \n",
       "\n",
       "   whether he/she donated blood in March 2007  \n",
       "0                                           1  \n",
       "1                                           1  \n",
       "2                                           1  \n",
       "3                                           1  \n",
       "4                                           0  \n",
       "5                                           0  \n",
       "6                                           1  \n",
       "7                                           0  \n",
       "8                                           1  \n",
       "9                                           1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(\"transfusion.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features from Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build X and y matrices\n",
    "X = df.drop(['whether he/she donated blood in March 2007'], axis=1)\n",
    "y = df[['whether he/she donated blood in March 2007']].values.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recency (months)                              0\n",
       "Frequency (times)                             0\n",
       "Monetary (c.c. blood)                         0\n",
       "Time (months)                                 0\n",
       "whether he/she donated blood in March 2007    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure there is no nan\n",
    "# if there is nan, you need to deal with it, either by imputing or discarding\n",
    "df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup\n",
    "\n",
    "Had the above test ( or any others wey may want to add ) had encountered issues we need to address, a lot more code could be required here...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "The stratify argument is used to make sure the train test split data has similar populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick A Model For A Base Point To Evaluate Other Models Against\n",
    "\n",
    "In this case we are choosing Logistric Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Predictor: Pipeline ==========\n",
      "Training result: f1: 0.201, acc: 0.774\n",
      "Test result: f1: 0.186, acc: 0.767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try LogisticRegression to establish a baseline performance\n",
    "pipeline = Pipeline([\n",
    "    ('scale', StandardScaler()), # remember to scale first before feeding data into lgr\n",
    "    ('lgr', LogisticRegression()),\n",
    "])\n",
    "evaluate(pipeline, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let's Try A Few More..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ander\\anaconda3\\envs\\mlenv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:42:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "========== Predictor: XGBClassifier ==========\n",
      "Training result: f1: 0.798, acc: 0.916\n",
      "Test result: f1: 0.471, acc: 0.760\n",
      "\n",
      "========== Predictor: LGBMClassifier ==========\n",
      "Training result: f1: 0.672, acc: 0.871\n",
      "Test result: f1: 0.464, acc: 0.753\n",
      "\n",
      "========== Predictor: RandomForestClassifier ==========\n",
      "Training result: f1: 0.854, acc: 0.938\n",
      "Test result: f1: 0.388, acc: 0.727\n",
      "\n",
      "========== Predictor: GradientBoostingClassifier ==========\n",
      "Training result: f1: 0.597, acc: 0.851\n",
      "Test result: f1: 0.467, acc: 0.787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try other predictors\n",
    "evaluate(XGBClassifier(n_jobs=-1), X_train, X_test, y_train, y_test)\n",
    "evaluate(LGBMClassifier(n_jobs=-1), X_train, X_test, y_train, y_test)\n",
    "evaluate(RandomForestClassifier(n_jobs=-1), X_train, X_test, y_train, y_test)\n",
    "evaluate(GradientBoostingClassifier(), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Pick a Final Model To Move Forward With\n",
    "\n",
    "From the above evaluations, it looks like XGBClassifier is a very promising candidate\n",
    "\n",
    "We will then hypertune the classifier model to come up with the best model we can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Create Our Tuning Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV on XGB\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [10, 20, 50, 100, 200, 300, 400],\n",
    "    'max_depth': np.arange(5, 20),\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'subsample': np.arange(0.5, 1.0, 0.05),\n",
    "    'min_child_weight': np.arange(1, 10),\n",
    "    'objective': ['reg:squarederror'],\n",
    "    'colsample_bytree': np.arange(0.2, 1.0, 0.1),\n",
    "    'gamma': [0, 0.001, 0.002, 0.003, 0.004, 0.005, 1e-2],\n",
    "    'n_jobs': [-1]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's find The Best Model We Can\n",
    "\n",
    "The RandomizedSearchCV function will try all our combinations above and select the most accurate model.  \n",
    "\n",
    "That best model is found in the best_estimator_ property of the RandomizedSerachCV object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "========== Predictor: XGBClassifier ==========\n",
      "Training result: f1: 0.543, acc: 0.831\n",
      "Test result: f1: 0.400, acc: 0.760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictor = XGBClassifier(use_label_encoder=False)\n",
    "rs = RandomizedSearchCV(predictor, xgb_param_grid, cv=5, scoring='f1', n_jobs=-1, n_iter=100, verbose=1)\n",
    "rs.fit(X_train, y_train)\n",
    "evaluate(rs.best_estimator_, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Our Model Further\n",
    "\n",
    "Now we are going to shuffle the data over and over and apply our new model to the results to further determine if we want to use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 0.78 (0.13) accuracy\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with kfold\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(rs.best_estimator_, X, y, cv=kfold, n_jobs=-1)\n",
    "print(\"Results: %.2f (%.2f) accuracy\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Model For Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "with open('best_xgb_model.pickle', 'wb') as f:\n",
    "    pickle.dump(rs.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm File\n",
    "\n",
    "Make sure the operating system you are NOT using is commented out below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is BCD0-C992\n",
      "\n",
      " Directory of C:\\Users\\ander\\aautsa202106\\demos\\extra_ml_stuff\\00_Project_3_Type_Example\n",
      "\n",
      "11/05/2021  09:46 PM            57,150 best_xgb_model.pickle\n",
      "               1 File(s)         57,150 bytes\n",
      "               0 Dir(s)  786,124,849,152 bytes free\n"
     ]
    }
   ],
   "source": [
    "# windows\n",
    "! dir best_xgb*\n",
    "# mac / linux / Unix\n",
    "# ! ls -a best_xgb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlenv]",
   "language": "python",
   "name": "conda-env-mlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
