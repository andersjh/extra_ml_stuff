{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Machine Learning Example\n",
    "\n",
    "###  This example reflects what a simple manual process for comming up witn an effective model for a classificaiton problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE FUNCTION\n",
    "\n",
    "Since we will be trying lots of different models, it would be nice to have a single function that will evaluate all our models and provide a standardized reporting format.\n",
    "\n",
    "This will allow us to easily pick out the model we want to move forward with.\n",
    "\n",
    "This function takes in a model ( pipeline ) and our train test split data. From there it simply performes predictions and generates results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pipeline, X_train, X_test, y_train, y_test):\n",
    "    '''\n",
    "    Evaluate a pipeline on training and test datasets\n",
    "    '''    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_train_hat = pipeline.predict(X_train)\n",
    "    y_test_hat = pipeline.predict(X_test)\n",
    "    train_f1 = f1_score(y_train_hat, y_train)\n",
    "    train_acc = accuracy_score(y_train_hat, y_train)\n",
    "    test_f1 = f1_score(y_test_hat, y_test)\n",
    "    test_acc = accuracy_score(y_test_hat, y_test)\n",
    "\n",
    "    print(f\"========== Predictor: {type(pipeline).__name__} ==========\")\n",
    "    print(f\"Training result: f1: {train_f1:.3f}, acc: {train_acc:.3f}\")\n",
    "    print(f\"Test result: f1: {test_f1:.3f}, acc: {test_acc:.3f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA\n",
    "\n",
    "In this case we are reading in transfusion data.  With this data we are trying to predict in an individual has given blood on Marth 2007 based on specific features.\n",
    "\n",
    "#### The features are:\n",
    "- Recency  ->   How long since the individual last gave blood\n",
    "- Frequency -> How many times has the indivuaul give blood\n",
    "- Monetary -> Amount of usable blood given\n",
    "- Time -> How many months have they been given blood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>12500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3250</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>5000</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>6000</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1750</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3000</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2250</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>11500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recency (months)  Frequency (times)  Monetary (c.c. blood)  Time (months)  \\\n",
       "0                 2                 50                  12500             98   \n",
       "1                 0                 13                   3250             28   \n",
       "2                 1                 16                   4000             35   \n",
       "3                 2                 20                   5000             45   \n",
       "4                 1                 24                   6000             77   \n",
       "5                 4                  4                   1000              4   \n",
       "6                 2                  7                   1750             14   \n",
       "7                 1                 12                   3000             35   \n",
       "8                 2                  9                   2250             22   \n",
       "9                 5                 46                  11500             98   \n",
       "\n",
       "   whether he/she donated blood in March 2007  \n",
       "0                                           1  \n",
       "1                                           1  \n",
       "2                                           1  \n",
       "3                                           1  \n",
       "4                                           0  \n",
       "5                                           0  \n",
       "6                                           1  \n",
       "7                                           0  \n",
       "8                                           1  \n",
       "9                                           1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(\"transfusion.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features from Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build X and y matrices\n",
    "X = df.drop(['whether he/she donated blood in March 2007'], axis=1)\n",
    "y = df[['whether he/she donated blood in March 2007']].values.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recency (months)                              0\n",
       "Frequency (times)                             0\n",
       "Monetary (c.c. blood)                         0\n",
       "Time (months)                                 0\n",
       "whether he/she donated blood in March 2007    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure there is no nan\n",
    "# if there is nan, you need to deal with it, either by imputing or discarding\n",
    "df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup\n",
    "\n",
    "Had the above test ( or any others wey may want to add ) had encountered issues we need to address, a lot more code could be required here...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "The stratify argument is used to make sure the train test split data has similar populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick A Model For A Base Point To Evaluate Other Models Against\n",
    "\n",
    "In this case we are choosing Logistric Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Predictor: Pipeline ==========\n",
      "Training result: f1: 0.201, acc: 0.774\n",
      "Test result: f1: 0.186, acc: 0.767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try LogisticRegression to establish a baseline performance\n",
    "pipeline = Pipeline([\n",
    "    ('scale', StandardScaler()), # remember to scale first before feeding data into lgr\n",
    "    ('lgr', LogisticRegression()),\n",
    "])\n",
    "evaluate(pipeline, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let's Try A Few More..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Predictor: XGBClassifier ==========\n",
      "Training result: f1: 0.798, acc: 0.916\n",
      "Test result: f1: 0.471, acc: 0.760\n",
      "\n",
      "========== Predictor: LGBMClassifier ==========\n",
      "Training result: f1: 0.672, acc: 0.871\n",
      "Test result: f1: 0.464, acc: 0.753\n",
      "\n",
      "========== Predictor: RandomForestClassifier ==========\n",
      "Training result: f1: 0.855, acc: 0.938\n",
      "Test result: f1: 0.435, acc: 0.740\n",
      "\n",
      "========== Predictor: GradientBoostingClassifier ==========\n",
      "Training result: f1: 0.589, acc: 0.846\n",
      "Test result: f1: 0.467, acc: 0.787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try other predictors\n",
    "evaluate(XGBClassifier(n_jobs=-1), X_train, X_test, y_train, y_test)\n",
    "evaluate(LGBMClassifier(n_jobs=-1), X_train, X_test, y_train, y_test)\n",
    "evaluate(RandomForestClassifier(n_jobs=-1), X_train, X_test, y_train, y_test)\n",
    "evaluate(GradientBoostingClassifier(), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Pick a Final Model To Move Forward With\n",
    "\n",
    "From the above evaluations, it looks like XGBClassifier is a very promising candidate\n",
    "\n",
    "We will then hypertune the classifier model to come up with the best model we can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Create Our Tuning Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV on XGB\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [10, 20, 50, 100, 200, 300, 400],\n",
    "    'max_depth': np.arange(5, 20),\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'subsample': np.arange(0.5, 1.0, 0.05),\n",
    "    'min_child_weight': np.arange(1, 10),\n",
    "    'colsample_bytree': np.arange(0.2, 1.0, 0.1),\n",
    "    'gamma': [0, 0.001, 0.002, 0.003, 0.004, 0.005, 1e-2],\n",
    "    'n_jobs': [-1]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's find The Best Model We Can\n",
    "\n",
    "The RandomizedSearchCV function will try all our combinations above and select the most accurate model.  \n",
    "\n",
    "That best model is found in the best_estimator_ property of the RandomizedSerachCV object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 419 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    9.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Predictor: XGBClassifier ==========\n",
      "Training result: f1: 0.593, acc: 0.846\n",
      "Test result: f1: 0.462, acc: 0.767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictor = XGBClassifier()\n",
    "rs = RandomizedSearchCV(predictor, xgb_param_grid, cv=5, scoring='f1', n_jobs=-1, n_iter=100, verbose=1)\n",
    "rs.fit(X_train, y_train)\n",
    "evaluate(rs.best_estimator_, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Our Model Further\n",
    "\n",
    "Now we are going to shuffle the data over and over and apply our new model to the results to further determine if we want to use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 0.77 (0.13) accuracy\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with kfold\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(rs.best_estimator_, X, y, cv=kfold, n_jobs=-1)\n",
    "print(\"Results: %.2f (%.2f) accuracy\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Model For Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "with open(f'best_xgb_model.pickle', 'wb') as f:\n",
    "    pickle.dump(rs.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
